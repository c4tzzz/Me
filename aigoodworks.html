<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover" />
  <title>AI GoodWords | c4tz</title>
  <link rel="stylesheet" href="static/css/style.css" />
</head>
<body>
  <div class="page">
    <aside class="sidebar">
      <a class="brand" href="about.html">
        <img class="avatar" alt="avatar c4tz" src="https://m.media-amazon.com/images/I/511PnEDrRfL._AC_.jpg">
        <div>
          <h1>c4tz</h1>
          <p>Portfolio &amp; writeups</p>
        </div>
      </a>

      <nav class="snav">
        <ul>
          <li><a href="writeup.html">Write Up</a></li>
          <li><a href="portfolio.html">Portfolio</a></li>
          <li><a href="skills.html">Skills</a></li>
          <li><a href="about.html">About</a></li>
        </ul>
      </nav>

      <div class="social">
        <a href="https://tryhackme.com" target="_blank" rel="noopener">
          <img src="static/image/thm.png" alt="TryHackMe">
        </a>
        <a href="https://zindi.africa/users/c4tz" target="_blank" rel="noopener">
          <img src="static/image/zindi.png" alt="Zindi">
        </a>
        <a href="https://app.hackthebox.com" target="_blank" rel="noopener">
          <img src="static/image/htb.png" alt="Hack The Box">
        </a>
        <a href="https://www.root-me.org/c4tz" target="_blank" rel="noopener">
          <img src="static/image/rootme.png" alt="Root-Me">
        </a>
        <a href="https://github.com/c4tzzz" target="_blank" rel="noopener">
          <img src="static/image/github.png" alt="GitHub">
        </a>
      </div>
    </aside>

    <div class="content">
      <div class="topbar">
        <img class="avatar" alt="avatar c4tz" src="https://m.media-amazon.com/images/I/511PnEDrRfL._AC_.jpg">
        <strong>c4tz</strong>
      </div>

      <main class="main">
        <h2 class="page-title">AI Red Teamer — GoodWords</h2>

        <section class="prose">
          <p class="meta">
            Attaque GoodWords sur un classifieur Naive Bayes entraîné sur un mini corpus de critiques de films,
            puis comparaison avec un MLP (scikit-learn).
          </p>

          <!-- 1. Modélisation théorique -->
          <details open>
            <summary><strong>1. Modélisation théorique du Naive Bayes et comptage des mots</strong></summary>

            <p>
              On considère deux classes : <strong>positif (+1)</strong> et <strong>négatif (-1)</strong>.
              À partir des 8 phrases d’entraînement, on compte pour chaque mot le nombre
              d’occurrences dans chaque classe : <code>N<sub>w,+</sub></code> et <code>N<sub>w,-</sub></code>.
            </p>

            <div class="grid two">
              <figure>
                <img src="static/image/portfolio/aigoodworks/6516a8fa-cde5-49e8-8cc9-ffc093f06f6c.png"
                     alt="Tableau de comptage Nw,+ et Nw,-">
                <figcaption>Tableau de comptage des mots positifs / négatifs.</figcaption>
              </figure>
              <figure>
                <img src="static/image/portfolio/aigoodworks/5988084e-c7f4-4f63-aa6d-b26ae9b82c75.png"
                     alt="Notes manuscrites sur Naive Bayes">
                <figcaption>Rappel des formules de probabilité conditionnelle utilisées dans le modèle.</figcaption>
              </figure>
            </div>

            <div class="callout">
              <p>
                Dans le modèle multinomial Naive Bayes, la probabilité d’un mot <code>w</code> donné la classe
                positive s’écrit (avec lissage de Laplace) :
              </p>
              <p>
                <code>P(w | +1) = (N<sub>w,+</sub> + 1) / (N<sub>+</sub> + |V|)</code><br>
                <code>P(w | -1) = (N<sub>w,-</sub> + 1) / (N<sub>-</sub> + |V|)</code>
              </p>
              <p>
                où <code>N<sub>+</sub></code> et <code>N<sub>-</sub></code> sont les nombres totaux de mots dans
                chaque classe et <code>|V|</code> la taille du vocabulaire.
              </p>
            </div>
          </details>

          <!-- 2. Lissage de Laplace & exemple de phrase négative -->
          <details open>
            <summary><strong>2. Lissage de Laplace et exemple de phrase négative</strong></summary>

            <p>
              Le <strong>lissage de Laplace</strong> évite d’avoir des probabilités nulles pour des mots jamais vus
              dans une classe donnée. On ajoute 1 à chaque compteur de mot :
              même un mot inconnu garde une probabilité non nulle.
            </p>

            <div class="grid two">
              <figure>
                <img src="static/image/portfolio/aigoodworks/840d852b-1187-4f94-8deb-eaca0d38c2dd.png"
                     alt="Démonstration du dénominateur avec Laplace">
                <figcaption>Détail sur les dénominateurs <code>N<sub>±</sub> + |V|</code> après lissage.</figcaption>
              </figure>
              <figure>
                <img src="static/image/portfolio/aigoodworks/e962e2a2-ea86-4036-abd4-e3ba9f5bec04.png"
                     alt="Exemple de phrase négative ce film est mauvais">
                <figcaption>Calcul du score pour la phrase « ce film est mauvais ».</figcaption>
              </figure>
            </div>

            <div class="callout">
              <p>
                En multipliant les probabilités des mots (<code>ce</code>, <code>film</code>, <code>est</code>,
                <code>mauvais</code>) et en appliquant les priors
                <code>P(+1) = P(-1) = 1/2</code>, on obtient un score plus fort pour la classe
                <strong>négative</strong>. La phrase est donc classée comme négative, ce qui est cohérent pour un humain.
              </p>
            </div>
          </details>

          <!-- 3. Phrase attaquée (GoodWords) -->
          <details open>
            <summary><strong>3. Attaque GoodWords : faire basculer une phrase négative</strong></summary>

            <p>
              L’attaque GoodWords consiste à <strong>ajouter des mots très positifs</strong> (ici
              <code>bien</code> et <code>génial</code>) à une phrase négative :
            </p>
            <p>
              <code>« ce film est mauvais »</code> → <code>« ce film est mauvais bien et génial »</code>
            </p>

            <figure>
              <img src="static/image/portfolio/aigoodworks/51487033-e1c7-43f3-9f1e-2dbbbb6c79e2.png"
                   alt="Calcul de la phrase attaquée ce film est mauvais bien et génial">
              <figcaption>Calcul des scores Naive Bayes pour la phrase attaquée.</figcaption>
            </figure>

            <div class="callout">
              <p>
                Après ajout des mots <code>bien</code> et <code>génial</code>, le produit des probabilités
                <code>P(d | +1)</code> devient plus grand que <code>P(d | -1)</code>.
                Le modèle classe alors la phrase comme <strong>positive</strong>,
                alors qu’elle reste négative pour un humain.
              </p>
              <p>
                C’est exactement le principe d’une <strong>attaque GoodWords</strong> : exploiter le fait que
                le modèle additionne / multiplie des scores de mots sans vraiment comprendre le sens global.
              </p>
            </div>
          </details>

          <!-- 4. Implémentation expérimentale avec scikit-learn & MLP -->
          <h3>Implémentation expérimentale avec scikit-learn et réseau de neurones (MLP)</h3>

          <details open>
            <summary><strong>4.1 Installation rapide</strong></summary>
            <p>Sur une distribution type Debian / Ubuntu :</p>
            <pre><code class="language-bash">sudo apt update
sudo apt install python3-sklearn</code></pre>
          </details>

          <details>
            <summary><strong>4.2 Script <code>goodwords_nb.py</code></strong></summary>

            <p>
              Le script suivant entraîne un Naive Bayes multinomial sur le corpus, calcule les probabilités
              <code>P(w | classe)</code>, puis compare les deux phrases de test.
              Il entraîne ensuite un petit <strong>MLPClassifier</strong> et teste les mêmes phrases.
            </p>

            <p>
              Téléchargement direct : <a href="static/image/portfolio/aigoodworks/goodwords_nb.py">goodwords_nb.py</a>
            </p>

            <pre><code class="language-python">#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
import numpy as np

# =========================
# 1) Corpus d'entraînement
# =========================

docs = [
    "ce film est bien",          # +1
    "j adore ce film",           # +1
    "ce film est incroyable",    # +1
    "bon et genial",             # +1
    "ce film est nul",           # -1
    "je deteste ce film",        # -1
    "le film est affreux",       # -1
    "mauvais et horrible",       # -1
]

y = [1, 1, 1, 1, 0, 0, 0, 0]     # 1 = positif, 0 = negatif

# IMPORTANT : token_pattern pour garder les mots d'une lettre (j)
vect = CountVectorizer(
    lowercase=True,
    token_pattern=r"(?u)\b\w+\b"
)

X = vect.fit_transform(docs)
vocab = vect.get_feature_names_out()
print("Vocabulaire (|V| = {}):".format(len(vocab)))
print(vocab, "\n")

# =========================
# 2) Comptages par classe
# =========================

# somme des occurrences par classe
X_pos = X[np.array(y) == 1]
X_neg = X[np.array(y) == 0]

counts_pos = np.asarray(X_pos.sum(axis=0)).ravel()
counts_neg = np.asarray(X_neg.sum(axis=0)).ravel()

print("Comptages par mot (N_w,+) et (N_w,-):")
for w, c_pos, c_neg in zip(vocab, counts_pos, counts_neg):
    print(f"{w:10s}  Nw,+ = {c_pos:2d}   Nw,- = {c_neg:2d}")
print()

print("Total mots positifs :", counts_pos.sum())
print("Total mots négatifs :", counts_neg.sum(), "\n")

# =========================
# 3) Modèle Naive Bayes
# =========================

clf = MultinomialNB(alpha=1.0)   # Laplace = 1
clf.fit(X, y)

print("Priors de classe (P(c)) :")
print("log P(c)  :", clf.class_log_prior_)
print("P(c)      :", np.exp(clf.class_log_prior_), "\n")

# feature_log_prob_ : log P(w | c)
feat_log_prob = clf.feature_log_prob_
feat_prob = np.exp(feat_log_prob)

print("Probabilités P(w | classe) avec Laplace :")
print("mot       P(w|neg)   P(w|pos)")
for i, w in enumerate(vocab):
    p_neg = feat_prob[0, i]   # classe 0 = négatif
    p_pos = feat_prob[1, i]   # classe 1 = positif
    print(f"{w:10s}  {p_neg:7.4f}   {p_pos:7.4f}")
print()

# =========================
# 4) Tests des deux phrases
# =========================

test_docs = [
    "ce film est mauvais",
    "ce film est mauvais bien et genial"
]

X_test = vect.transform(test_docs)
probas = clf.predict_proba(X_test)
preds = clf.predict(X_test)

for doc, p, yhat in zip(test_docs, probas, preds):
    label = "positif" if yhat == 1 else "negatif"
    print(f"Phrase : « {doc} »")
    print(f"  P(negatif) = {p[0]:.4f}")
    print(f"  P(positif) = {p[1]:.4f}")
    print(f"  → prédiction sklearn : {label}")
    print()</code></pre>
          </details>

          <details>
            <summary><strong>4.3 Sorties console & visualisations</strong></summary>

            <p>Extrait des sorties :</p>
            <pre><code>Vocabulaire (|V| = 17):
['adore' 'affreux' 'bien' 'bon' 'ce' 'deteste' 'est' 'et' 'film' 'genial'
 'horrible' 'incroyable' 'j' 'je' 'le' 'mauvais' 'nul'] 

Matrice BoW (docs x mots) :
Lignes = documents, Colonnes = vocabulaire dans l'ordre ci-dessus

Doc 0 : « ce film est bien »
  vecteur = [0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0]

Doc 1 : « j adore ce film »
  vecteur = [1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0]

Doc 2 : « ce film est incroyable »
  vecteur = [0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0]

Doc 3 : « bon et genial »
  vecteur = [0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0]

Doc 4 : « ce film est nul »
  vecteur = [0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1]

Doc 5 : « je deteste ce film »
  vecteur = [0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0]

Doc 6 : « le film est affreux »
  vecteur = [0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0]

Doc 7 : « mauvais et horrible »
  vecteur = [0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0]

Comptages par mot (N_w,+) et (N_w,-):
adore       Nw,+ =  1   Nw,- =  0
affreux     Nw,+ =  0   Nw,- =  1
bien        Nw,+ =  1   Nw,- =  0
bon         Nw,+ =  1   Nw,- =  0
ce          Nw,+ =  3   Nw,- =  2
deteste     Nw,+ =  0   Nw,- =  1
est         Nw,+ =  2   Nw,- =  2
et          Nw,+ =  1   Nw,- =  1
film        Nw,+ =  3   Nw,- =  3
genial      Nw,+ =  1   Nw,- =  0
horrible    Nw,+ =  0   Nw,- =  1
incroyable  Nw,+ =  1   Nw,- =  0
j           Nw,+ =  1   Nw,- =  0
je          Nw,+ =  0   Nw,- =  1
le          Nw,+ =  0   Nw,- =  1
mauvais     Nw,+ =  0   Nw,- =  1
nul         Nw,+ =  0   Nw,- =  1

Total mots positifs : 15
Total mots négatifs : 15 

Priors de classe (P(c)) :
log P(c)  : [-0.69314718 -0.69314718]
P(c)      : [0.5 0.5] 

Probabilités P(w | classe) avec Laplace :
mot       P(w|neg)   P(w|pos)
adore        0.0312    0.0625
affreux      0.0625    0.0312
bien         0.0312    0.0625
bon          0.0312    0.0625
ce           0.0937    0.1250
deteste      0.0625    0.0312
est          0.0937    0.0937
et           0.0625    0.0625
film         0.1250    0.1250
genial       0.0312    0.0625
horrible     0.0625    0.0312
incroyable   0.0312    0.0625
j            0.0312    0.0625
je           0.0625    0.0312
le           0.0625    0.0312
mauvais      0.0625    0.0312
nul          0.0625    0.0312

Phrase : « ce film est mauvais »
  P(negatif) = 0.6000
  P(positif) = 0.4000
  → prédiction Naive Bayes : negatif

Phrase : « ce film est mauvais bien et genial »
  P(negatif) = 0.2727
  P(positif) = 0.7273
  → prédiction Naive Bayes : positif</code></pre>

            <p>
              En complément, un MLP est entraîné sur les mêmes vecteurs BoW. Sur le train il est parfait
              (8/8 bien classés) et reproduit aussi l’effet GoodWords sur les deux phrases de test.
            </p>

            <figure>
              <img src="static/image/portfolio/aigoodworks/image.png"
                   alt="Probabilités des phrases (Naive Bayes vs MLP)">
              <figcaption>Probabilités des deux phrases pour Naive Bayes et pour le MLP.</figcaption>
            </figure>
          </details>

          <!-- 5. Analyse et perspectives -->
          <h3>Analyse des résultats et perspectives</h3>

          <div class="grid two">
            <figure>
              <img src="static/image/portfolio/aigoodworks/image%201.png"
                   alt="Distribution P(w | classe) avec Laplace">
              <figcaption>
                Visualisation de <code>P(w | négatif)</code> vs <code>P(w | positif)</code> pour chaque mot du vocabulaire.
              </figcaption>
            </figure>
            <figure>
              <img src="static/image/portfolio/aigoodworks/image%202.png"
                   alt="Notes manuscrites d'analyse et limites">
              <figcaption>
                Notes sur les limites des modèles BoW (pas de contexte, addition naïve des mots) et sur les pistes
                de durcissement vis-à-vis des attaques GoodWords.
              </figcaption>
            </figure>
          </div>

          <div class="callout">
            <p>
              Même un modèle plus complexe (MLP) reste fragile si on l’entraîne sur des
              représentations trop simples (BoW). Les LLM modernes gèrent mieux le contexte,
              mais peuvent eux aussi être attaqués via des variantes de GoodWords / prompt injection,
              d’où l’importance de la <strong>red team AI</strong> pour tester la robustesse.
            </p>
          </div>
        </section>
      </main>

      <footer class="footer">© 2025 c4tz — respect cats</footer>
    </div>
  </div>

  <nav class="mobile-nav" aria-label="Navigation mobile">
    <a href="writeup.html"><span>WriteUp</span></a>
    <a href="portfolio.html"><span>Portfolio</span></a>
    <a href="skills.html"><span>Skills</span></a>
    <a href="about.html"><span>About</span></a>
  </nav>

  <!-- helpers de mise en page simple (si non déjà dans ton CSS) -->
  <style>
    .prose img{max-width:100%;height:auto;border-radius:8px}
    .grid.two{display:grid;gap:1rem;grid-template-columns:repeat(2,minmax(0,1fr))}
    @media (max-width: 800px){.grid.two{grid-template-columns:1fr}}
    .callout{padding:1rem;border-radius:8px;background:rgba(255,255,255,.04);border:1px solid rgba(255,255,255,.08)}
    .meta{opacity:.7;font-size:.9rem;margin-bottom:.5rem}
    pre{overflow:auto;padding:.75rem;border-radius:8px;background:rgba(127,127,127,.12)}
    code{font-family:ui-monospace,SFMono-Regular,Menlo,Consolas,monospace}
    figure{margin:1.25em 0}
    figcaption{opacity:.7;font-size:.85rem;margin-top:.4rem}
  </style>
</body>
</html>
